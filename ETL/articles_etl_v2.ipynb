{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-20-17.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6-amzn-0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f31740eae50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer,\n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import concat, col, lit, spark_partition_id, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3 bucket location of the csv file\n",
    "input_path = 's3://largedatabucket/*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually set the schema for the csv file instead of inferring\n",
    "schema = StructType([\n",
    "    StructField(\"_c0\", IntegerType()),\n",
    "    StructField(\"Unnamed: 0\", IntegerType()),\n",
    "    StructField(\"date\", DateType()),\n",
    "    StructField(\"year\", IntegerType()),\n",
    "    StructField(\"month\", FloatType()),\n",
    "    StructField(\"day\", IntegerType()),\n",
    "    StructField(\"author\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"article\", StringType()),\n",
    "    StructField(\"url\", StringType()),\n",
    "    StructField(\"section\", StringType()),\n",
    "    StructField(\"publication\", StringType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").option(\"encoding\", \"UTF-8\").csv(input_path, schema=schema, header='true').withColumnRenamed(\"Unnamed: 0\",\"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a temp table to query and clean\n",
    "df.createOrReplaceTempView('articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data that only has actual articles\n",
    "df = spark.sql('''SELECT uid,\n",
    "                         date,\n",
    "                         year,\n",
    "                         month,\n",
    "                         day,\n",
    "                         author,\n",
    "                         title,\n",
    "                         article,\n",
    "                         url,\n",
    "                         publication\n",
    "                   FROM articles \n",
    "                   WHERE article IS NOT NULL''')\n",
    "\n",
    "#fill in missing information\n",
    "df = df.fillna({'date':'1970-01-01 00:00:00',\n",
    "                'year':1970,\n",
    "                'month':1.0,\n",
    "                'day':1.0,\n",
    "                'author':'missing',\n",
    "                'title':'missing',\n",
    "                'url':'missing',\n",
    "                'publication':'missing'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only data existing in the 2000s\n",
    "df = spark.sql('''\n",
    "                    SELECT *\n",
    "                    FROM articles\n",
    "                    WHERE year >= 2000\n",
    "                    AND year <2021\n",
    "\n",
    "               ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set manual stop words from NLTK stopwords\n",
    "stopwords = ['a','about','above','after','again','against','ain','all','also','am','an','and','any','are','aren',\"aren't\",'as','at','be','because','been','before','being','below','between','both','breitbart','but',\n",
    " 'by','can','cnn','could','couldn',\"couldn't\",'d','dent','did','didn',\"didn't\",'didnt','do','does','doesn',\"doesn't\",'doing','don',\"don't\",'dont','down','during','each','edu','few','for','fox','from',\n",
    " 'further','get','go','going','had','hadn',\"hadn't\",'has','hasn',\"hasn't\",'have','haven',\"haven't\",'having','he','her','here','hers','herself','him','himself','his','how','i','if','in','into','is','isn',\n",
    " \"isn't\",'it',\"it's\",'its','itself','just','kind','like','ll','m','ma','maybe','me','mightn',\"mightn't\",'more','most','mustn',\"mustn't\",'my','myself','needn',\"needn't\",'no','nor','not','now','o','of',\n",
    " 'off','on','once','only','or','other','our','ours','ourselves','out','over','own','re','s','said','same','say','says','shan',\"shan't\",'she',\"she's\",'should',\"should've\",'shouldn',\"shouldn't\",'so','some',\n",
    " 'still','subject','such','t','than','that',\"that'll\",'thats','the','their','theirs','them','themselves','then','there','theres','these','they','thing','things','think','this','those','through','to','too',\n",
    " 'u','under','until','up','use','ve','very','wanted','was','wasn',\"wasn't\",'way','we','went','were','weren',\"weren't\",'what','when','where','which','while','who','whom','why','will','with','won',\"won't\",'would',\n",
    " 'wouldn',\"wouldn't\",'y','you',\"you'd\",\"you'll\",\"you're\",\"you've\",'your','yours','yourself','yourselves', 'reuters', 'source', 'coverage','eikon', 'gdynia', 'baht','ltd', 'ago','bangalorenewsroomthomsonreuterscom',\n",
    " 'ab', 'mr', 'inc', 'co','youre', 'without','try','know','im', 'doesnt','become','always','already','source','ltd','trumptrump','tel','nw','hes','fy' ,'far','fax','top','without','want','though','tell'\n",
    "            'something','since','really','put','ms','im','hes','already','tel','bi',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "#parse all the articles in the 'article' column\n",
    "#have it output to a new column called document\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('article') \\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "#parse the document column and tokenize the text\n",
    "#output to a new column called 'token'\n",
    "tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['document']) \\\n",
    "     .setOutputCol('token')\n",
    "\n",
    "#Remove punctuation, numbers, and symbols from text from tokens\n",
    "#output to a new column called 'normalized'\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['token']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)\n",
    "\n",
    "#get the root of each word by performing lemmatization\n",
    "#output to a new column called lemma\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('lemma')\n",
    "\n",
    "#remove all stopwords from lemma column\n",
    "#ouptut to a new column called 'clean_lemma'\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemma']) \\\n",
    "     .setOutputCol('clean_lemma') \\\n",
    "     .setCaseSensitive(False) \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['clean_lemma']) \\\n",
    "     .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "     .setStages([\n",
    "           documentAssembler,\n",
    "           tokenizer,\n",
    "           normalizer,\n",
    "           lemmatizer,\n",
    "           stopwords_cleaner,\n",
    "           finisher\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = pipe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the final column to all strings so it can be written to CSV\n",
    "clean = clean.withColumn(\"bow\", clean[\"finished_clean_lemma\"].cast(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only the columns we care about\n",
    "results = clean.select('uid',\n",
    "                       'date',\n",
    "                       'year',\n",
    "                       'month',\n",
    "                       'day',\n",
    "                       'author',\n",
    "                       'title',\n",
    "                       'article',\n",
    "                       'publication',\n",
    "                       'bow'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.withColumn('month', results['month'].cast(IntegerType())).withColumn('uid', concat(col(\"month\"), lit(\".\"), col(\"day\"), lit(\".\"), col(\"uid\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.write.format(\"csv\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").option(\"encoding\", \"UTF-8\").option(\"header\",\"true\").mode(\"Overwrite\").save(\"s3://largedatabucket/clean2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
